{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-01 20:18:57.865270: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib:/usr/local/lib:/usr/local/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/srv/conda/envs/notebook/lib:/usr/local/harris/idl88/bin/bin.linux.x86_64\n",
      "2022-08-01 20:18:57.865314: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import glob\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from keras.models import Sequential, Input, Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout, Flatten \n",
    "from keras.layers import Conv1D, MaxPooling1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Configuration variables and path setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 573300\n"
     ]
    }
   ],
   "source": [
    "csvPath = '/efs/tnarock/fluxrope_data/cc_noise/'\n",
    "\n",
    "allFiles = glob.glob( csvPath + \"*.csv\" )\n",
    "random.shuffle(allFiles)\n",
    "print(\"Dataset size:\", len(allFiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset size: 426300\n"
     ]
    }
   ],
   "source": [
    "subsetFiles = []\n",
    "for file in allFiles:\n",
    "    parts = file.split('/')\n",
    "    fname = parts[-1]\n",
    "    parts = fname.split('_')\n",
    "    y0 = int(parts[6])\n",
    "    if ( (y0 > -75) and (y0 < 75) ):\n",
    "        subsetFiles.append(file)\n",
    "print(\"Subset size:\", len(subsetFiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training files: 255780\n",
      "Test files: 85260\n",
      "Validation files: 85260\n",
      "\n",
      "Total number of files: 426300\n"
     ]
    }
   ],
   "source": [
    "# Distribution is 60% Training, 20% Validation, and 20% Testing\n",
    "cutOff = int(len(subsetFiles) * 0.6)\n",
    "train = subsetFiles[:cutOff]\n",
    "rest = subsetFiles[cutOff:]\n",
    "\n",
    "# there's 40% left over, split it in half for testing and validation\n",
    "test = rest[ : int(len(rest) * 0.5)]\n",
    "validation = rest[ int(len(rest) * 0.5) : ]\n",
    "\n",
    "print(\"Training files:\", len(train))\n",
    "print(\"Test files:\", len(test))\n",
    "print(\"Validation files:\", len(validation))\n",
    "print()\n",
    "print(\"Total number of files:\", len(subsetFiles))\n",
    "\n",
    "# check to make sure we split everything correctly\n",
    "total = len(test) + len(train) + len(validation)\n",
    "assert total == len(subsetFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCSV(files):\n",
    "    \n",
    "    count = len(files)\n",
    "    dataX = np.zeros(shape=(count, 50, 3))\n",
    "    \n",
    "    p = np.zeros( shape=(count,2) )\n",
    "    t = np.zeros( shape=(count,2) )\n",
    "    y0 = np.zeros( count )\n",
    "    h = np.zeros( count )\n",
    "    \n",
    "    index = 0\n",
    "    for f in files:\n",
    "        \n",
    "        # extract the output value from the file name\n",
    "        parts = f.split(\"/\")\n",
    "        fname = parts[-1]\n",
    "        parts = fname.split(\"_\")   \n",
    "        \n",
    "        del parts[2] #Adjustment made only for noise data, convention purposes\n",
    "        \n",
    "        phi = float(parts[3])\n",
    "        theta = float(parts[4])\n",
    "        y0R = float(parts[5])\n",
    "        HH = parts[6]\n",
    "        parts = HH.split(\".\")\n",
    "        HH = float(parts[0])\n",
    "        \n",
    "        y0[index] = y0R\n",
    "        \n",
    "        p[index, 0] = np.sin( np.radians(phi) )\n",
    "        p[index, 1] = np.cos( np.radians(phi) )\n",
    "        \n",
    "        t[index, 0] = np.sin( np.radians(theta) )\n",
    "        t[index, 1] = np.cos( np.radians(theta) )\n",
    "                \n",
    "        # change handedness from -1 or 1 to 0 or 1 to align with \n",
    "        # neural network sigmoid function\n",
    "        if (HH == -1):\n",
    "            h[index] = 0\n",
    "        else:\n",
    "            h[index] = 1\n",
    "                    \n",
    "        # read the data\n",
    "        bx = []\n",
    "        by = []\n",
    "        bz = []\n",
    "        openFile = open(f, \"r\")\n",
    "        for line in openFile:\n",
    "            parts = line.split(\",\")\n",
    "            bx.append(float(parts[0]))\n",
    "            by.append(float(parts[1]))\n",
    "            bz.append(float(parts[2]))\n",
    "        openFile.close()\n",
    "        \n",
    "        bx = np.array(bx)\n",
    "        by = np.array(by)\n",
    "        bz = np.array(bz)\n",
    "        b = np.sqrt( bx*bx + by*by + bz*bz )\n",
    "        phi = np.degrees(np.arctan2(bx,by))\n",
    "        theta = np.degrees(np.arcsin( bz/b ))\n",
    "\n",
    "        dataX[index, :, 0] = bx\n",
    "        dataX[index, :, 1] = by\n",
    "        dataX[index, :, 2] = bz\n",
    "        \n",
    "        index += 1\n",
    "\n",
    "    return dataX, p, t, y0, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def makeNpzFiles(fileList, batchSize, outDir):\n",
    "    \n",
    "    # make sure there are no npz files left over from a previous run\n",
    "    c = 0\n",
    "    for filename in os.listdir(outDir):\n",
    "        file_path = os.path.join(outDir, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "                c += 1\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
    "\n",
    "    print(\"Deleted\", c, \"npz files in\", outDir)\n",
    "        \n",
    "    loops = int(np.ceil(len(fileList)/batchSize))\n",
    "    index1 = 0\n",
    "    index2 = batchSize\n",
    "    numFiles = 0\n",
    "    for l in range(loops):\n",
    "        files = fileList[index1:index2]\n",
    "\n",
    "        x, p, t, y0, h = readCSV(files)\n",
    "        if (l < 10):\n",
    "            counter = '00' + str(l)\n",
    "        elif ( (l >= 10) and (l < 100) ):\n",
    "            counter = '0' + str(l)\n",
    "        else:\n",
    "            counter = str(l)\n",
    "        outfile = outDir + 'fluxropes_' + counter + '.npz'\n",
    "        np.savez(outfile, x=x, phi=p, theta=t, y0=y0, h=h, files=files)\n",
    "        index1 = index2\n",
    "        index2 += batchSize\n",
    "        numFiles += 1\n",
    "    print(\"Created\", numFiles, \"npz files in\", outDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 0 npz files in /efs/tnarock/y0_data/npz_training/\n",
      "Created 1000 npz files in /efs/tnarock/y0_data/npz_training/\n",
      "Deleted 0 npz files in /efs/tnarock/y0_data/npz_validation/\n",
      "Created 334 npz files in /efs/tnarock/y0_data/npz_validation/\n",
      "Deleted 0 npz files in /efs/tnarock/y0_data/npz_testing/\n",
      "Created 334 npz files in /efs/tnarock/y0_data/npz_testing/\n"
     ]
    }
   ],
   "source": [
    "makeNpzFiles(train, 256, '/efs/tnarock/y0_data/npz_training/')\n",
    "makeNpzFiles(validation, 256, '/efs/tnarock/y0_data/npz_validation/')\n",
    "makeNpzFiles(test, 256, '/efs/tnarock/y0_data/npz_testing/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = glob.glob( \"/efs/tnarock/y0_data/npz_training/*.npz\" )\n",
    "validation = glob.glob( \"/efs/tnarock/y0_data/npz_validation/*.npz\" )\n",
    "testing = glob.glob( \"/efs/tnarock/y0_data/npz_testing/*.npz\" )\n",
    "\n",
    "random.shuffle(train)\n",
    "random.shuffle(validation)\n",
    "random.shuffle(testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    \n",
    "    def __init__(self, files, batch_size=1, shuffle=False):\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.files = files\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        'Denotes the number of batches per epoch'\n",
    "\n",
    "        'If the batch size doesnt divide evenly then add 1'\n",
    "        diff = (len(self.files) / self.batch_size) - np.floor((len(self.files) / self.batch_size))\n",
    "        if ( diff > 0 ):\n",
    "            return int(np.floor(len(self.files) / self.batch_size))+1\n",
    "        else:\n",
    "            return int(np.floor(len(self.files) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # get list of files\n",
    "        files = [self.files[k] for k in indexes]\n",
    "\n",
    "        npzfile = np.load(files[0])\n",
    "        x = npzfile['x']\n",
    "        p = npzfile['phi']\n",
    "        t = npzfile['theta']\n",
    "        y0 = npzfile['y0']/100.\n",
    "        h = npzfile['h']\n",
    "        \n",
    "                  \n",
    "        #y = {\"phi\": p, \"theta\": t, \"y0\": y0, \"handedness\": h}\n",
    "        \n",
    "        size = p.shape[0]\n",
    "        \n",
    "        y = np.zeros(shape=(size,6))\n",
    "        y[:,0] = p[:,0]\n",
    "        y[:,1] = p[:,1]\n",
    "        y[:,2] = t[:,0]\n",
    "        y[:,3] = t[:,1]\n",
    "        y[:,4] = y0[:]\n",
    "        y[:,5] = h[:]\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.files))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import sin as sin\n",
    "from numpy import cos as cos\n",
    "import warnings, traceback, sys\n",
    "\n",
    "def ec_create_data(delta,\n",
    "    phi,\n",
    "    theta,\n",
    "    psi,\n",
    "    y0,\n",
    "    h,\n",
    "    C10=1,\n",
    "    By0=10,\n",
    "    us = 450.0,\n",
    "    R=0.07,\n",
    "    tau=1.3,\n",
    "    n = 1,\n",
    "    m = 0,\n",
    "    norm_poloidal=True,\n",
    "    noise_type='none',\n",
    "    epsilon=0.05,\n",
    "    num=50):\n",
    "    \"\"\"\n",
    "    The Elliptic-cylindrical Magnetic Flux Rope Model\n",
    "\n",
    "    A python implementation of the model by T. Nieves-Chinchilla et al. from\n",
    "    https://doi.org/10.3847/1538-4357/aac951\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    All angles are expected in units of degrees.\n",
    "\n",
    "    Coordinate system described are with the assumption that the \n",
    "    spacecraft trajectory is along the GSEx direction.\n",
    "    (TODO: generalize these descriptions to reflect a relationship\n",
    "    with the S/C RTN system)\n",
    "\n",
    "    delta : float, A measure of the ellipticity of the flux rope.  \n",
    "            The ratio between the length of the minor and major axes\n",
    "            of the elliptical cross section. Valid range (0,1]\n",
    "    \n",
    "    phi : float, Angle of rotations of the flux rope around the Z axis, in \n",
    "          the XY plane. Valid range [0,360]\n",
    "    \n",
    "    theta : float, Angle of rotation of the flux rope out of the XY plane. \n",
    "            Valid range [-90, 90]\n",
    "    \n",
    "    psi : float, Angle of rotation about the central axis of the flux rope.\n",
    "          Valid range [0,180]\n",
    "    \n",
    "    y0 : float, Spacecraft distance from the central axis of the flux rope.\n",
    "         Valid range (-100, 100)\n",
    "    \n",
    "    h :  int, {-1, 1} Helicty of flux rope.\n",
    "\n",
    "    C10 : int, optional, default: 1\n",
    "         Valid only if > 0\n",
    "\n",
    "    By0 : int, optional, default: 10\n",
    "\n",
    "    tau : float, optional, default: 1.3\n",
    "\n",
    "    us : float, optional, default: 450.0\n",
    "         Spacecraft velocity in km/s\n",
    "\n",
    "    R : float, optional, default: 0.07\n",
    "        Radius of the semi-major axis in Astronomical Units (AU)\n",
    "\n",
    "    n : float, optional, default: 1.0\n",
    "\n",
    "    m : float, optional, default: 0.0\n",
    "\n",
    "    norm_poloidal : boolean, optional, default: True\n",
    "        If True, divide poloidal components by Bmax\n",
    "\n",
    "    noise_type : {'none,'gaussian','uniform'}, optional, default: 'none'\n",
    "        The type of noise to add to each component of the magnetic field \n",
    "        result.\n",
    "\n",
    "    epsilon : float > 0, optional, default: 0.05\n",
    "        Size of noise modifier.  Ignored if noise_type is 'none'.\n",
    "        \n",
    "        For noise_type 'gaussian', epsilon is the standard deviation of the \n",
    "        normal distrubution centered on 0.  Values in the distribution\n",
    "        Normal(mu=0,sigma=epsilon) are added to the magnetic field components.\n",
    "\n",
    "        For noise_type 'uniform', epsilon defines the +/- bounds of the \n",
    "        distribution.  Values in [-epsilon,epsilon] are added to the magnetic\n",
    "        field components.\n",
    "\n",
    "    num : int, optional, default: 50\n",
    "        Length of return data time series in points.\n",
    "    \"\"\"\n",
    "\n",
    "    # Store values for warning messages\n",
    "    phi_in = phi\n",
    "    th_in  = theta\n",
    "    psi_in = psi\n",
    "    y0_in  = y0\n",
    "\n",
    "    #assert delta <= 1 and delta > 0, \"DELTA must be in (0,1]\"\n",
    "    #assert phi > 0 and phi < 360, \"PHI must be in (0,360)\"\n",
    "    #assert phi != 180, \"PHI cannot be 180\"\n",
    "    #assert theta >= -90 and theta <= 90, \"THETA must be in [-90,90]\"\n",
    "    #assert psi >= 0 and psi <= 180, \"PSI must be in [0,180]\"\n",
    "    #assert y0 > -100 and y0 < 100, \"Y0 must be in (-100,100)\"\n",
    "    y0[y0 < -100] = -100\n",
    "    y0[y0 >  100] =  100\n",
    "    \n",
    "    #assert h == 1 or h == -1, \"H must be in {-1, 1}\"\n",
    "    #assert C10 > 0, \"C10 must be > 0\"\n",
    "    # TODO: error checks for By0, tau and R ranges\n",
    "\n",
    "    # Constants\n",
    "    AU_to_km = 1.5e8\n",
    "    AU_to_m  = 1.5e11\n",
    "\n",
    "    # Derived parameters\n",
    "    if norm_poloidal:\n",
    "        Bmax = delta * By0 * tau\n",
    "    else:\n",
    "        Bmax = 1.\n",
    "\n",
    "    # Adjust arguments for internal coordinate system and convert to radians\n",
    "    phi = np.radians(phi-90.)\n",
    "    #phi = tf.angle(phi-90.)\n",
    "    \n",
    "    #if (phi < 0):\n",
    "        #phi += 2 * np.pi\n",
    "    phi[phi < 0] = phi + 2*np.pi    \n",
    "        \n",
    "    theta = np.radians(theta)\n",
    "    psi = np.radians(psi)\n",
    "    y0 = y0*R/100.\n",
    "\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings('error')\n",
    "        try:\n",
    "            #TODO: Give BGSE a more generic name.  \n",
    "            # My understanding is it's essentially a rotation of the RTN system.\n",
    "\n",
    "            HH_factor = np.sqrt(1 - (sin(phi) * cos(theta))**2)\n",
    "            z0 = y0 * HH_factor / (cos(theta) * cos(phi))\n",
    "\n",
    "            fctr1 = cos(psi)**2 + (delta * sin(psi))**2 # b-term in the notes\n",
    "            fctr2 = sin(psi)**2 + (delta * cos(psi))**2 # a-term in the notes\n",
    "            fctr3 = delta**2 - 1\n",
    "\n",
    "            F2 = fctr1 * cos(phi)**2 \\\n",
    "                + fctr2 * (sin(phi) * sin(theta))**2 \\\n",
    "                + 2 * fctr3 * cos(phi)  * sin(phi) * sin(theta) * cos(psi) * sin(psi)\n",
    "\n",
    "            F1 = fctr2 * sin(phi) * sin(theta) * cos(theta) \\\n",
    "                + fctr3 * cos(phi) * cos(theta) * sin(psi) * cos(psi)\n",
    "\n",
    "            time_final = AU_to_km * (2./us) * (1./F2) \\\n",
    "                        * np.sqrt(F2 * R**2 - (HH_factor*y0)**2)\n",
    "            \n",
    "            tt = np.linspace(0,time_final,num=num)\n",
    "\n",
    "            xc = (500 * time_final * us / AU_to_m) - (z0 * F1 / F2)\n",
    "            AA = -xc * (cos(phi) * cos(psi) - sin(phi) * sin(theta) * sin(psi)) \\\n",
    "                + cos(theta) * sin(psi) * z0\n",
    "\n",
    "            BB = -xc * (cos(phi) * sin(psi) + sin(phi) * sin(theta) * cos(psi)) \\\n",
    "                - cos(theta) * cos(psi) * z0\n",
    "\n",
    "            RR = np.sqrt((AA/delta)**2 + BB**2)\n",
    "\n",
    "            xsat = (1.0e3 * us * tt / AU_to_m) - xc\n",
    "\n",
    "            XL = xsat * (cos(phi) * cos(psi) - sin(phi) * sin(theta) * sin(psi)) \\\n",
    "                + cos(theta)*sin(psi)*z0\n",
    "            YL = -sin(phi) * cos(theta) * xsat + sin(theta) * z0\n",
    "            ZL =  xsat * (cos(phi) * sin(psi) + sin(phi) * sin(theta) * cos(psi)) \\\n",
    "                - cos(theta) * cos(psi) * z0\n",
    "            rnm = np.sqrt(XL**2 + ZL**2)\n",
    "            PHIsat = np.arctan2(ZL,XL)\n",
    "            HH_f = np.sqrt((delta * sin(phi))**2 + cos(phi)**2)\n",
    "            HR = np.sqrt(sin(phi)**2 + (delta * cos(phi))**2)\n",
    "\n",
    "            By = delta * By0 * (tau - (rnm/RR)**(n+1)) / Bmax\n",
    "            fctr = (n+1) / (delta**2 + m + 1)\n",
    "            Bpol = -h * delta * HH_f * fctr * (By0 / C10) * (rnm/RR)**(m+1) / Bmax\n",
    "\n",
    "            if noise_type == 'fractal':\n",
    "                if (num%2)==0:\n",
    "                    frac_by,_ = fractal_synth(hpar=0.354844, std=1.321449, nfreq=num/2, verbose=False)\n",
    "                    frac_bpol,_ = fractal_synth(hpar=0.319066, std=1.270344, nfreq=num/2, verbose=False)\n",
    "                else:\n",
    "                    raise ValueError('If noise_type == \"fractal\" you need to select num such as num/2 is in integer!')\n",
    "                \n",
    "                By = By + frac_by\n",
    "                Bpol = Bpol + frac_bpol\n",
    "                \n",
    "            BxL = -Bpol * sin(PHIsat)\n",
    "            ByL = By\n",
    "            BzL = Bpol * cos(PHIsat)\n",
    "\n",
    "            BxGSE = (BxL * (cos(phi) * cos(psi) - sin(psi) * sin(phi) *sin(theta)) \n",
    "                - ByL * sin(phi) * cos(theta) \n",
    "                + BzL * (cos(phi) * sin(psi) + cos(psi) * sin(phi) * sin(theta)))\n",
    "\n",
    "            ByGSE = BxL* (sin(phi) * cos(psi) + sin(psi) * cos(phi) *sin(theta)) \\\n",
    "                + ByL * cos(phi)*cos(theta) \\\n",
    "                + BzL * (sin(phi) * sin(psi) - cos(psi) * cos(phi) * sin(theta))\n",
    "\n",
    "            BzGSE = -BxL * sin(psi) * cos(theta) \\\n",
    "                + ByL * sin(theta) \\\n",
    "                + BzL * cos(psi) * cos(theta)\n",
    "\n",
    "\n",
    "            # Add the noise\n",
    "            if noise_type == 'uniform':\n",
    "                rng = np.random.default_rng()   #Random number Generator\n",
    "                BxGSE = 2 * epsilon * rng.random(num) - epsilon\n",
    "                ByGSE = 2 * epsilon * rng.random(num) - epsilon\n",
    "                BzGSE = 2 * epsilon * rng.random(num) - epsilon\n",
    "\n",
    "            elif noise_type == 'gaussian':\n",
    "                rng = np.random.default_rng()   #Random number Generator\n",
    "                BxGSE += rng.normal(0,epsilon,num)\n",
    "                ByGSE += rng.normal(0,epsilon,num)\n",
    "                BzGSE += rng.normal(0,epsilon,num)\n",
    "\n",
    "            BB = np.sqrt(BxGSE**2 +ByGSE**2 + BzGSE**2)\n",
    "\n",
    "        except Warning:\n",
    "            exc_type, exc_value, exc_traceback = sys.exc_info()\n",
    "            # traceback.print_tb(exc_traceback, limit=1, file=sys.stdout)\n",
    "            traceback.print_exc(limit=1, file=sys.stdout)\n",
    "            print(\"Above- delta:{d}; phi:{ph}; theta:{th}; psi:{ps}; y0:{y0}; h:{h}\".format(d=delta,ph=phi_in,th=th_in,ps=psi_in,y0=y0_in,h=h))\n",
    "            return(np.full((5,num),np.nan))\n",
    "\n",
    "    #return(np.stack((BxGSE,ByGSE,BzGSE)))\n",
    "    return BxGSE,ByGSE,BzGSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.losses import binary_crossentropy\n",
    "from scipy.stats import pearsonr \n",
    "\n",
    "def getCorrelation( x, y ):\n",
    "\n",
    "    mx = tf.math.reduce_mean(x)\n",
    "    my = tf.math.reduce_mean(y)\n",
    "    xm, ym = x-mx, y-my\n",
    "    r_num = tf.math.reduce_mean(tf.multiply(xm,ym))        \n",
    "    r_den = tf.math.reduce_std(xm) * tf.math.reduce_std(ym)\n",
    "    return r_num / r_den \n",
    "        \n",
    "    \n",
    "def custom_loss():\n",
    "    def my_custom_loss(y_true, y_pred):\n",
    "\n",
    "        gamma = 0.01\n",
    "        \n",
    "        phi_sin = K.mean(K.square(y_pred[:, 0] - y_true[:, 0])) \n",
    "        phi_cos = K.mean(K.square(y_pred[:, 1] - y_true[:, 1])) \n",
    "        theta_sin = K.mean(K.square(y_pred[:, 2] - y_true[:, 2])) \n",
    "        theta_cos = K.mean(K.square(y_pred[:, 3] - y_true[:, 3])) \n",
    "        y0 = K.mean(K.square(y_pred[:, 4] - y_true[:, 4])) \n",
    "        h = binary_crossentropy(y_true[:,5], y_pred[:,5])\n",
    "        \n",
    "        phi_predicted = tf.math.atan2(y_pred[:, 0], y_pred[:, 1])\n",
    "        theta_predicted = tf.math.atan2(y_pred[:, 2], y_pred[:, 3])\n",
    "        \n",
    "        phi_predicted = tf.Variable(phi_predicted).numpy()\n",
    "        theta_predicted = tf.Variable(theta_predicted).numpy()\n",
    "        y0_predicted = tf.Variable(y_pred[:,4]).numpy()\n",
    "        h_predicted = tf.Variable(y_pred[:,5]).numpy()\n",
    "        \n",
    "        phi_true = tf.math.atan2(y_true[:, 0],y_true[:, 1])\n",
    "        theta_true = tf.math.atan2(y_true[:, 2], y_true[:, 3])\n",
    "        \n",
    "        phi_true = tf.Variable(phi_true).numpy()\n",
    "        theta_true = tf.Variable(theta_true).numpy()\n",
    "        y0_true = tf.Variable(y_true[:,4]).numpy()\n",
    "        h_true = tf.Variable(y_true[:,5]).numpy()\n",
    "        \n",
    "        bx_p, by_p, bz_p = ec_create_data(1, phi_predicted, theta_predicted, 0, y0_predicted, h_predicted)\n",
    "        bx_t, by_t, bz_t = ec_create_data(1, phi_true, theta_true, 0, y0_true, h_true)\n",
    "\n",
    "        rx = getCorrelation( bx_p, bx_t )\n",
    "        ry = getCorrelation( by_p, by_t )\n",
    "        rz = getCorrelation( bz_p, bz_t )\n",
    "        correlation = 3. - (rx + ry + rz)\n",
    "        \n",
    "        return K.sum(phi_sin + phi_cos + theta_sin + theta_cos + y0 + h + (gamma*correlation))\n",
    "    return my_custom_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "    \n",
    "input_shape = (50, 3)\n",
    "\n",
    "def get_model(act=\"relu\"):\n",
    "    \n",
    "    inp = Input(input_shape)\n",
    "    x = Conv1D(32, kernel_size=5, activation=act, padding=\"same\")(inp)\n",
    "    x = Conv1D(64, kernel_size=5, activation=act, padding=\"same\")(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Dropout(0.1)(x, training=False) # 0.25\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation=act)(x)\n",
    "    x = Dropout(0.2)(x, training=False) # 0.50\n",
    "    out = Dense(6, activation=\"tanh\")(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "\n",
    "    model.compile(loss=custom_loss(), optimizer='adam', run_eagerly=True)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 50, 3)]           0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 50, 32)            512       \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 50, 64)            10304     \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 25, 64)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 25, 64)            0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1600)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               204928    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 6)                 774       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 216,518\n",
      "Trainable params: 216,518\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-01 20:19:21.398571: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib:/usr/local/lib:/usr/local/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/srv/conda/envs/notebook/lib:/usr/local/harris/idl88/bin/bin.linux.x86_64\n",
      "2022-08-01 20:19:21.398623: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-08-01 20:19:21.398651: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (jupyter-tnarock): /proc/driver/nvidia/version does not exist\n",
      "2022-08-01 20:19:21.400005: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# clear out any old models hanging around\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# define model\n",
    "model = get_model()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "1000/1000 [==============================] - 121s 120ms/step - loss: 0.2678 - val_loss: 0.1225 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "1000/1000 [==============================] - 121s 121ms/step - loss: 0.0897 - val_loss: 0.0727 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "1000/1000 [==============================] - 121s 121ms/step - loss: 0.0753 - val_loss: 0.0769 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0630 - val_loss: 0.0533 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0731 - val_loss: 0.5341 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0963 - val_loss: 0.0747 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0649 - val_loss: 0.0517 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0517 - val_loss: 0.0519 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0586 - val_loss: 0.0429 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0401 - val_loss: 0.0378 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0370 - val_loss: 0.0352 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0349 - val_loss: 0.0356 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0332 - val_loss: 0.0337 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0389 - val_loss: 0.0368 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0308 - val_loss: 0.0381 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0622 - val_loss: 0.0580 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0349 - val_loss: 0.0301 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0297 - val_loss: 0.0296 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0396 - val_loss: 0.0309 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0327 - val_loss: 0.0288 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0288 - val_loss: 0.0273 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "1000/1000 [==============================] - 121s 121ms/step - loss: 0.0340 - val_loss: 0.0331 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0498 - val_loss: 0.0334 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0309 - val_loss: 0.0294 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0273 - val_loss: 0.0377 - lr: 0.0010\n",
      "Epoch 26/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0464 - val_loss: 0.0459 - lr: 0.0010\n",
      "Epoch 27/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0331 - val_loss: 0.0312 - lr: 0.0010\n",
      "Epoch 28/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0290 - val_loss: 0.0292 - lr: 0.0010\n",
      "Epoch 29/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0298 - val_loss: 0.0267 - lr: 0.0010\n",
      "Epoch 30/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0252 - val_loss: 0.0272 - lr: 0.0010\n",
      "Epoch 31/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0258 - val_loss: 0.0378 - lr: 0.0010\n",
      "Epoch 32/500\n",
      "1000/1000 [==============================] - 121s 121ms/step - loss: 0.0273 - val_loss: 0.0269 - lr: 0.0010\n",
      "Epoch 33/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0231 - val_loss: 0.0251 - lr: 0.0010\n",
      "Epoch 34/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0444 - val_loss: 0.0432 - lr: 0.0010\n",
      "Epoch 35/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0334 - val_loss: 0.0284 - lr: 0.0010\n",
      "Epoch 36/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0250 - val_loss: 0.0273 - lr: 0.0010\n",
      "Epoch 37/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0428 - val_loss: 0.0673 - lr: 0.0010\n",
      "Epoch 38/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0363 - val_loss: 0.0348 - lr: 0.0010\n",
      "Epoch 39/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0639 - val_loss: 0.0404 - lr: 0.0010\n",
      "Epoch 40/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0355 - val_loss: 0.0289 - lr: 0.0010\n",
      "Epoch 41/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0244 - val_loss: 0.0283 - lr: 0.0010\n",
      "Epoch 42/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0325 - val_loss: 0.0362 - lr: 0.0010\n",
      "Epoch 43/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0248 - val_loss: 0.0279 - lr: 0.0010\n",
      "Epoch 44/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0197 - val_loss: 0.0223 - lr: 5.0000e-04\n",
      "Epoch 45/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0195 - val_loss: 0.0226 - lr: 5.0000e-04\n",
      "Epoch 46/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0198 - val_loss: 0.0228 - lr: 5.0000e-04\n",
      "Epoch 47/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0218 - val_loss: 0.0238 - lr: 5.0000e-04\n",
      "Epoch 48/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0193 - val_loss: 0.0213 - lr: 5.0000e-04\n",
      "Epoch 49/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0186 - val_loss: 0.0216 - lr: 5.0000e-04\n",
      "Epoch 50/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0194 - val_loss: 0.0214 - lr: 5.0000e-04\n",
      "Epoch 51/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0214 - val_loss: 0.0457 - lr: 5.0000e-04\n",
      "Epoch 52/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0236 - val_loss: 0.0278 - lr: 5.0000e-04\n",
      "Epoch 53/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0190 - val_loss: 0.0221 - lr: 5.0000e-04\n",
      "Epoch 54/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0182 - val_loss: 0.0222 - lr: 5.0000e-04\n",
      "Epoch 55/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0204 - val_loss: 0.0272 - lr: 5.0000e-04\n",
      "Epoch 56/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0178 - val_loss: 0.0202 - lr: 5.0000e-04\n",
      "Epoch 57/500\n",
      "1000/1000 [==============================] - 119s 119ms/step - loss: 0.0172 - val_loss: 0.0229 - lr: 5.0000e-04\n",
      "Epoch 58/500\n",
      "1000/1000 [==============================] - 119s 119ms/step - loss: 0.0193 - val_loss: 0.0204 - lr: 5.0000e-04\n",
      "Epoch 59/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0173 - val_loss: 0.0202 - lr: 5.0000e-04\n",
      "Epoch 60/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0170 - val_loss: 0.0221 - lr: 5.0000e-04\n",
      "Epoch 61/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0177 - val_loss: 0.0215 - lr: 5.0000e-04\n",
      "Epoch 62/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0174 - val_loss: 0.0219 - lr: 5.0000e-04\n",
      "Epoch 63/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0172 - val_loss: 0.0203 - lr: 5.0000e-04\n",
      "Epoch 64/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0177 - val_loss: 0.0207 - lr: 5.0000e-04\n",
      "Epoch 65/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0175 - val_loss: 0.0312 - lr: 5.0000e-04\n",
      "Epoch 66/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0174 - val_loss: 0.0208 - lr: 5.0000e-04\n",
      "Epoch 67/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0153 - val_loss: 0.0187 - lr: 2.5000e-04\n",
      "Epoch 68/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0152 - val_loss: 0.0185 - lr: 2.5000e-04\n",
      "Epoch 69/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0153 - val_loss: 0.0189 - lr: 2.5000e-04\n",
      "Epoch 70/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0152 - val_loss: 0.0183 - lr: 2.5000e-04\n",
      "Epoch 71/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0151 - val_loss: 0.0186 - lr: 2.5000e-04\n",
      "Epoch 72/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0150 - val_loss: 0.0183 - lr: 2.5000e-04\n",
      "Epoch 73/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0151 - val_loss: 0.0184 - lr: 2.5000e-04\n",
      "Epoch 74/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0150 - val_loss: 0.0184 - lr: 2.5000e-04\n",
      "Epoch 75/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0149 - val_loss: 0.0182 - lr: 2.5000e-04\n",
      "Epoch 76/500\n",
      "1000/1000 [==============================] - 126s 126ms/step - loss: 0.0149 - val_loss: 0.0187 - lr: 2.5000e-04\n",
      "Epoch 77/500\n",
      "1000/1000 [==============================] - 121s 121ms/step - loss: 0.0148 - val_loss: 0.0181 - lr: 2.5000e-04\n",
      "Epoch 78/500\n",
      "1000/1000 [==============================] - 128s 128ms/step - loss: 0.0147 - val_loss: 0.0180 - lr: 2.5000e-04\n",
      "Epoch 79/500\n",
      "1000/1000 [==============================] - 123s 123ms/step - loss: 0.0148 - val_loss: 0.0183 - lr: 2.5000e-04\n",
      "Epoch 80/500\n",
      "1000/1000 [==============================] - 121s 121ms/step - loss: 0.0147 - val_loss: 0.0182 - lr: 2.5000e-04\n",
      "Epoch 81/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0146 - val_loss: 0.0184 - lr: 2.5000e-04\n",
      "Epoch 82/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0146 - val_loss: 0.0185 - lr: 2.5000e-04\n",
      "Epoch 83/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0145 - val_loss: 0.0178 - lr: 2.5000e-04\n",
      "Epoch 84/500\n",
      "1000/1000 [==============================] - 125s 125ms/step - loss: 0.0145 - val_loss: 0.0181 - lr: 2.5000e-04\n",
      "Epoch 85/500\n",
      "1000/1000 [==============================] - 121s 121ms/step - loss: 0.0145 - val_loss: 0.0182 - lr: 2.5000e-04\n",
      "Epoch 86/500\n",
      "1000/1000 [==============================] - 121s 121ms/step - loss: 0.0144 - val_loss: 0.0178 - lr: 2.5000e-04\n",
      "Epoch 87/500\n",
      "1000/1000 [==============================] - 130s 130ms/step - loss: 0.0154 - val_loss: 0.0177 - lr: 2.5000e-04\n",
      "Epoch 88/500\n",
      "1000/1000 [==============================] - 123s 123ms/step - loss: 0.0143 - val_loss: 0.0183 - lr: 2.5000e-04\n",
      "Epoch 89/500\n",
      "1000/1000 [==============================] - 122s 122ms/step - loss: 0.0142 - val_loss: 0.0178 - lr: 2.5000e-04\n",
      "Epoch 90/500\n",
      "1000/1000 [==============================] - 142s 143ms/step - loss: 0.0143 - val_loss: 0.0178 - lr: 2.5000e-04\n",
      "Epoch 91/500\n",
      "1000/1000 [==============================] - 123s 123ms/step - loss: 0.0142 - val_loss: 0.0181 - lr: 2.5000e-04\n",
      "Epoch 92/500\n",
      "1000/1000 [==============================] - 122s 122ms/step - loss: 0.0142 - val_loss: 0.0178 - lr: 2.5000e-04\n",
      "Epoch 93/500\n",
      "1000/1000 [==============================] - 126s 126ms/step - loss: 0.0142 - val_loss: 0.0177 - lr: 2.5000e-04\n",
      "Epoch 94/500\n",
      "1000/1000 [==============================] - 179s 179ms/step - loss: 0.0142 - val_loss: 0.0179 - lr: 2.5000e-04\n",
      "Epoch 95/500\n",
      "1000/1000 [==============================] - 177s 177ms/step - loss: 0.0141 - val_loss: 0.0176 - lr: 2.5000e-04\n",
      "Epoch 96/500\n",
      "1000/1000 [==============================] - 178s 178ms/step - loss: 0.0141 - val_loss: 0.0176 - lr: 2.5000e-04\n",
      "Epoch 97/500\n",
      "1000/1000 [==============================] - 182s 182ms/step - loss: 0.0141 - val_loss: 0.0174 - lr: 2.5000e-04\n",
      "Epoch 98/500\n",
      "1000/1000 [==============================] - 182s 182ms/step - loss: 0.0140 - val_loss: 0.0185 - lr: 2.5000e-04\n",
      "Epoch 99/500\n",
      "1000/1000 [==============================] - 181s 181ms/step - loss: 0.0140 - val_loss: 0.0180 - lr: 2.5000e-04\n",
      "Epoch 100/500\n",
      "1000/1000 [==============================] - 162s 162ms/step - loss: 0.0141 - val_loss: 0.0172 - lr: 2.5000e-04\n",
      "Epoch 101/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0142 - val_loss: 0.0185 - lr: 2.5000e-04\n",
      "Epoch 102/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0139 - val_loss: 0.0184 - lr: 2.5000e-04\n",
      "Epoch 103/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0139 - val_loss: 0.0176 - lr: 2.5000e-04\n",
      "Epoch 104/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0139 - val_loss: 0.0175 - lr: 2.5000e-04\n",
      "Epoch 105/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0138 - val_loss: 0.0178 - lr: 2.5000e-04\n",
      "Epoch 106/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0140 - val_loss: 0.0172 - lr: 2.5000e-04\n",
      "Epoch 107/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0138 - val_loss: 0.0179 - lr: 2.5000e-04\n",
      "Epoch 108/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0138 - val_loss: 0.0174 - lr: 2.5000e-04\n",
      "Epoch 109/500\n",
      "1000/1000 [==============================] - 123s 123ms/step - loss: 0.0137 - val_loss: 0.0174 - lr: 2.5000e-04\n",
      "Epoch 110/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0138 - val_loss: 0.0177 - lr: 2.5000e-04\n",
      "Epoch 111/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0132 - val_loss: 0.0168 - lr: 1.2500e-04\n",
      "Epoch 112/500\n",
      "1000/1000 [==============================] - 119s 119ms/step - loss: 0.0132 - val_loss: 0.0169 - lr: 1.2500e-04\n",
      "Epoch 113/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0132 - val_loss: 0.0168 - lr: 1.2500e-04\n",
      "Epoch 114/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0132 - val_loss: 0.0171 - lr: 1.2500e-04\n",
      "Epoch 115/500\n",
      "1000/1000 [==============================] - 121s 121ms/step - loss: 0.0132 - val_loss: 0.0170 - lr: 1.2500e-04\n",
      "Epoch 116/500\n",
      "1000/1000 [==============================] - 121s 121ms/step - loss: 0.0132 - val_loss: 0.0168 - lr: 1.2500e-04\n",
      "Epoch 117/500\n",
      "1000/1000 [==============================] - 121s 121ms/step - loss: 0.0132 - val_loss: 0.0167 - lr: 1.2500e-04\n",
      "Epoch 118/500\n",
      "1000/1000 [==============================] - 121s 121ms/step - loss: 0.0131 - val_loss: 0.0169 - lr: 1.2500e-04\n",
      "Epoch 119/500\n",
      "1000/1000 [==============================] - 128s 128ms/step - loss: 0.0131 - val_loss: 0.0170 - lr: 1.2500e-04\n",
      "Epoch 120/500\n",
      "1000/1000 [==============================] - 122s 122ms/step - loss: 0.0131 - val_loss: 0.0168 - lr: 1.2500e-04\n",
      "Epoch 121/500\n",
      "1000/1000 [==============================] - 123s 123ms/step - loss: 0.0132 - val_loss: 0.0170 - lr: 1.2500e-04\n",
      "Epoch 122/500\n",
      "1000/1000 [==============================] - 123s 123ms/step - loss: 0.0130 - val_loss: 0.0168 - lr: 1.0000e-04\n",
      "Epoch 123/500\n",
      "1000/1000 [==============================] - 129s 129ms/step - loss: 0.0130 - val_loss: 0.0167 - lr: 1.0000e-04\n",
      "Epoch 124/500\n",
      "1000/1000 [==============================] - 129s 129ms/step - loss: 0.0130 - val_loss: 0.0168 - lr: 1.0000e-04\n",
      "Epoch 125/500\n",
      "1000/1000 [==============================] - 134s 134ms/step - loss: 0.0130 - val_loss: 0.0168 - lr: 1.0000e-04\n",
      "Epoch 126/500\n",
      "1000/1000 [==============================] - 121s 121ms/step - loss: 0.0129 - val_loss: 0.0168 - lr: 1.0000e-04\n",
      "Epoch 127/500\n",
      "1000/1000 [==============================] - 121s 121ms/step - loss: 0.0130 - val_loss: 0.0166 - lr: 1.0000e-04\n",
      "Epoch 128/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0129 - val_loss: 0.0169 - lr: 1.0000e-04\n",
      "Epoch 129/500\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 0.0129 - val_loss: 0.0168 - lr: 1.0000e-04\n",
      "Epoch 130/500\n",
      "1000/1000 [==============================] - 121s 121ms/step - loss: 0.0129 - val_loss: 0.0167 - lr: 1.0000e-04\n",
      "Epoch 131/500\n",
      "1000/1000 [==============================] - 121s 121ms/step - loss: 0.0129 - val_loss: 0.0168 - lr: 1.0000e-04\n",
      "Epoch 132/500\n",
      "1000/1000 [==============================] - 121s 121ms/step - loss: 0.0129 - val_loss: 0.0166 - lr: 1.0000e-04\n",
      "Epoch 133/500\n",
      "1000/1000 [==============================] - 121s 121ms/step - loss: 0.0129 - val_loss: 0.0167 - lr: 1.0000e-04\n",
      "Epoch 134/500\n",
      "1000/1000 [==============================] - 123s 123ms/step - loss: 0.0129 - val_loss: 0.0167 - lr: 1.0000e-04\n",
      "Epoch 135/500\n",
      "1000/1000 [==============================] - 126s 126ms/step - loss: 0.0129 - val_loss: 0.0167 - lr: 1.0000e-04\n",
      "Epoch 136/500\n",
      "1000/1000 [==============================] - 123s 123ms/step - loss: 0.0128 - val_loss: 0.0168 - lr: 1.0000e-04\n",
      "Epoch 137/500\n",
      "1000/1000 [==============================] - 125s 125ms/step - loss: 0.0128 - val_loss: 0.0168 - lr: 1.0000e-04\n",
      "Epoch 138/500\n",
      "1000/1000 [==============================] - 121s 121ms/step - loss: 0.0128 - val_loss: 0.0168 - lr: 1.0000e-04\n",
      "Epoch 139/500\n",
      "1000/1000 [==============================] - 121s 121ms/step - loss: 0.0128 - val_loss: 0.0168 - lr: 1.0000e-04\n",
      "Epoch 140/500\n",
      "1000/1000 [==============================] - 127s 127ms/step - loss: 0.0128 - val_loss: 0.0169 - lr: 1.0000e-04\n",
      "Epoch 141/500\n",
      "1000/1000 [==============================] - 136s 136ms/step - loss: 0.0128 - val_loss: 0.0168 - lr: 1.0000e-04\n",
      "Epoch 142/500\n",
      "1000/1000 [==============================] - 121s 121ms/step - loss: 0.0129 - val_loss: 0.0171 - lr: 1.0000e-04\n",
      "Epoch 142: early stopping\n",
      "Fit took 295.46 minutes\n",
      "Fit took 4.92 hours\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "epochs = 500\n",
    "trainGen = DataGenerator(train, batch_size=1)\n",
    "valGen = DataGenerator(validation, batch_size=1)\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        \"/efs/tnarock/regression_noise_y0_revised.h5\", \\\n",
    "        save_best_only=True, monitor=\"val_loss\"\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5, patience=10, min_lr=0.0001\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=15, verbose=1),\n",
    "]\n",
    "\n",
    "model.fit(trainGen, epochs=epochs, shuffle=True, verbose=1, \\\n",
    "          validation_data=(valGen), callbacks=callbacks) \n",
    "\n",
    "stop = time.time()\n",
    "duration = stop-start\n",
    "print(\"Fit took\", round(duration/60,2), \"minutes\")\n",
    "print(\"Fit took\", round(duration/60/60,2), \"hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Loss - 1D CNN')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEICAYAAAC55kg0AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAA8y0lEQVR4nO3deXhU5dnH8e+TZJKQPSSsCQQCyCY7KoqKa11wQbFV0CrFV6utr7W2Fm3dra362lZptWrrglupdSsq7riwuQAKgoDskIBk38ieed4/7pnMJExCMkmYmcz9ua5ckznnzJlnopzfPOsx1lqUUkqFp4hAF0AppVTgaAgopVQY0xBQSqkwpiGglFJhTENAKaXCmIaAUkqFMQ0BpZQKYxoCKqgYY3YaY04LdDmaM8b0M8YsMsbsNcZYY8ygZvs/NsZUG2PKjTFlxpjVxpibjTExhzjv0caYxcaYEmNMkTHmC2PMT1z7TnK91yPNXrPMGDPH9fsc1zE3NTsmxxhzUsc/ueruNASUahsn8A4ws5VjrrPWJgL9gF8BlwCLjTHG18HGmGOBJcAnwFAgDbgWOMvrsAPA5c1Dp5kiYJ4xJqltH0UpDw0BFRKMMTHGmIdc38T3un6Pce1LN8a86fVteqkxJsK1b54xJtf1DX2zMeZUf97fWrvfWvso8GUbjj1grf0YOA84FpjewqH/Byyw1t5vrS2wYrW19kdex5QAzwB3tPKWG4GVwC8P+UGUakZDQIWK3wFTgPHAOOBo4FbXvl8BOUAvoA/wW8AaY4YD1wFHub6hnwHsPFwFttbuBlYBJzTfZ4yJQwLi5Tac6l5gpuvztOQ24JfGmJ7+lFWFLw0BFSouBe621uZZa/OBu4Afu/bVIU0wWdbaOmvtUiuLYjUAMcAoY4zDWrvTWrvtMJd7L+DrwpyK/Pvbd6gTWGu/Bx4D7m7lmK+B94B5fpVShS0NARUq+gO7vJ7vcm0DaVbZCrxnjNlujLkZwFq7FbgBuBPIM8YsNMb0pxljzEBjTIX7p5PLnYG02TdXjPQz9Gvjee4HzjDGjGvlmNuBa40xfdtXRBXONARUqNgLZHk9H+jahrW23Fr7K2ttNnAucKO77d9a+6K19njXay1yMW3CWrvbWpvg/umsAhtjBgCTgKU+3rMSacdvraPZ+/hC4CHgnlaO2QS8ijSHKdUmGgIqGDmMMbFeP1HAv4BbjTG9jDHpyLfe5wGMMecYY4a6RuGUIc1ADcaY4caYU1wdyNVAlWufX4wxsUjzEkCM67mv4+KMMdOA/wJfAItbOOVvgDnGmJuMMWmu144zxixs4fg/A8cBI1sp5l3AT4CU1j6LUm4aAioYLUYu2O6fO4HfI52s64BvgDWubQDDgA+ACuTb9aOu0TkxwH1AAfA90JuOfUuucr0HwCbXc29/M8aUA/uRb+2vAGdaa52+TmatXQGc4vrZbowpAp6ghdCw1pYBD+C7j8F9zA7gOSC+bR9JhTujN5VRSqnwpTUBpZQKYxoCSikVxjQElFIqjGkIKKVUGIsKdAFak56ebgcNGhToYiilVMhYvXp1gbW2V1uPD+oQGDRoEKtWrQp0MZRSKmQYY3Yd+igPbQ5SSqkwpiGglFJhLChDwBhzrjHmidLS0kAXRSmlurWg7BOw1r4BvDF58uSrAl0WpVTnqKurIycnh+rq6kAXpVuIjY0lMzMTh8PRofMEZQgopbqfnJwcEhMTGTRoEC3ccVO1kbWWwsJCcnJyGDx4cIfOFZTNQUqp7qe6upq0tDQNgE5gjCEtLa1TalUaAkqpw0YDoPN01t8ytENg82b46KNAl0IppUJWaIfAfffB//xPoEuhlAoRCQmdduO4biO0Q6CqSn6UUkr5JbRDoK4OamsDXQqlVAj7+uuvmTJlCmPHjuWCCy6guLgYgPnz5zNq1CjGjh3LJZdcAsAnn3zC+PHjGT9+PBMmTKC8vDyQRe8UoT1EVENAqZB0ww3w9dede87x4+Ghh9r/ussvv5y//vWvTJs2jdtvv5277rqLhx56iPvuu48dO3YQExNDSUkJAA8++CCPPPIIU6dOpaKigthYn7eZDilBWRNo84zhujr5UUopP5SWllJSUsK0adMAuOKKK/j0008BGDt2LJdeeinPP/88UVHyfXnq1KnceOONzJ8/n5KSksbtoSwoP0GbZwxrTUCpkOTPN/bD7a233uLTTz9l0aJF3HPPPWzYsIGbb76Z6dOns3jxYqZMmcIHH3zAiBEjAl3UDgnKmkCb1dWB0wkNDYEuiVIqBCUnJ5OamsrSpUsBeO6555g2bRpOp5M9e/Zw8skn88ADD1BSUkJFRQXbtm1jzJgxzJs3j8mTJ7Np06YAf4KOC8qaQJu5m4Jqa6FHj8CWRSkV9CorK8nMzGx8fuONN7JgwQKuueYaKisryc7O5umnn6ahoYHLLruM0tJSrLX88pe/JCUlhdtuu42PPvqIyMhIRo0axVlnnRXAT9M5NASUUmHD6XT63P7ZZ58dtG3ZsmUHbfvrX//a6WUKtNBvDvJ+VEop1S7dIwS0c1gppfyiIaCUUmFMQ0AppcJYaIdAfb08aggopZRfQjsEtGNYKaU6pHuEgNYElFKHcNJJJ/Huu+822fbQQw/xs5/9rMXjV61aBcDZZ5/duH6QtzvvvJMHH3yw1fd9/fXX+fbbbxuf33777XzwwQftLH3XCcoQaNfaQaAhoJQ6pFmzZrFw4cIm2xYuXMisWbMO+drFixeTkpLi1/s2D4G7776b0047za9zdYWgDAFr7RvW2quTk5NbP1BDQCnVRhdddBFvvvkmNTU1AOzcuZO9e/fy4osvMnnyZEaPHs0dd9zh87WDBg2ioKAAgHvvvZfhw4dz2mmnsXnz5sZj/vGPf3DUUUcxbtw4Zs6cSWVlJStWrGDRokXcdNNNjB8/nm3btjFnzhxefvllAD788EMmTJjAmDFjmDt3bmPZBg0axB133MHEiRMZM2ZMly5P0X1mDCulQkcA1pJOS0vj6KOP5p133uH8889n4cKFXHzxxdxyyy307NmThoYGTj31VNatW8fYsWN9nmP16tUsXLiQr776ivr6eiZOnMikSZMAuPDCC7nqKlnz8tZbb+XJJ5/kf//3fznvvPM455xzuOiii5qcq7q6mjlz5vDhhx9yxBFHcPnll/P3v/+dG264AYD09HTWrFnDo48+yoMPPsg///nPDv+JfAnKmkCbWOtZOE47hpVSbeDdJORuCnrppZeYOHEiEyZMYMOGDU2abppbunQpF1xwAXFxcSQlJXHeeec17lu/fj0nnHACY8aM4YUXXmDDhg2tlmXz5s0MHjyYI444Ami6jDVIqABMmjSJnTt3+vuRDyl0awLeF36tCSgVWgK0lvSMGTO48cYbWbNmDVVVVaSmpvLggw/y5Zdfkpqaypw5c6iurm71HMYYn9vnzJnD66+/zrhx43jmmWf4+OOPWz2PtbbV/TExMQBERkZS7x4O3wVCtyagIaCUaqeEhAROOukk5s6dy6xZsygrKyM+Pp7k5GT279/P22+/3errTzzxRF577TWqqqooLy/njTfeaNxXXl5Ov379qKur44UXXmjcnpiY6PM2lCNGjGDnzp1s3boV8CxjfbhpTUApFVZmzZrFhRdeyMKFCxkxYgQTJkxg9OjRZGdnM3Xq1FZfO3HiRC6++GLGjx9PVlYWJ5xwQuO+e+65h2OOOYasrCzGjBnTeOG/5JJLuOqqq5g/f35jhzBAbGwsTz/9ND/84Q+pr6/nqKOO4pprrumaD90Kc6gqSSBNnjzZusfpHiQ/H3r3lt8ffxyuvvrwFUwp1W4bN25k5MiRgS5Gt+Lrb2qMWW2tndzWc3SP5iDtGFZKKb90jxDQ5iCllPKLhoBS6rAJ5ubnUNNZf0sNAaXUYREbG0thYaEGQSew1lJYWEhsbGyHz6Wjg5RSh0VmZiY5OTnk5+cHuijdQmxsLJmZmR0+T+iGgPfkCe0YViroORwOBg8eHOhiqGa0OUgppcJYUIZAm5aS1hBQSqkOC8oQaNNS0hoCSinVYUEZAm2iIaCUUh3WPUJAO4aVUsov3SMEtCaglFJ+CeoQaHU4sTsEIiM1BJRSyk9BHQLuG4f55A6B+HgNAaWU8lNQh4DT2cpO7xDQPgGllPJL9wgBrQkopZRfNASUUiqMBXUItKlPIC5OQ0AppfwU1CGgNQGllOpa3SMEtGNYKaX8Eroh4F5KWmsCSinlt9ANAfe3/x49NASUUspPoR0CUVEQE6MhoJRSfgrtEHA4IDpaQ0AppfzUPUJAO4aVUsovoR8CDofWBJRSyk9BHwItBoF3TaCh4RAzy5RSSvkS1CEAUF3dwg7vEHA/V0op1S5BGQLuG80DVFa2cFDzENAmIaWUaregDAH3jeYBDhxo4SD3EFGHw/NcKaVUuwRlCHjTmoBSSnWdoA+BVmsCGgJKKdUhQR8CWhNQSqmuoyGglFJhLOhD4JDNQdoxrJRSfgv6ENCagFJKdZ2gD4EWawL19RoCSinVQUEfAloTUEqprhP0IaBDRJVSqusEdQgY04aagHYMK6WU34I6BCIitDlIKaW6UtCHgDYHKaVU1wn6ENCagFJKdZ2gD4E2TxbTEFBKqXYL+hBoc01AO4aVUqrdgj4EtE9AKaW6TtCHgPYJKKVU1wnqEIiMbCEE3Hegj4rSEFBKqQ4I6hBosTnI3f7vcEhSgIaAUkr5IehDwGdNwDsEjJHagHYMK6VUuwV9CByyJgASAloTUEqpdgv6EKir8/ElX0NAKaU6RdCHAPhoEqqvl0cNAaWU6pCgDoGYugrARwg0rwk4HBoCSinlh6AOgejaNoaAdgwrpZRfgjoEIpzS7HNQ57D2CSilVKcI6hCIdMrFvk01AQ0BpZRqt6AOAdPQxpqA9gkopZRfgjoEIhq0JqCUUl0pqEOgzTUB7RhWSim/hEAIWK0JKKVUFwnqEMBakinVEFBKqS5y2ELAGJNtjHnSGPNye17XmzztGFZKqS7SphAwxjxljMkzxqxvtv1MY8xmY8xWY8zNrZ3DWrvdWntlewvYmzytCSilVBeJauNxzwB/A551bzDGRAKPAKcDOcCXxphFQCTwx2avn2utzfOngAOi21AT0I5hpZTyS5tCwFr7qTFmULPNRwNbrbXbAYwxC4HzrbV/BM7xt0DGmKuBqwEmAf0d+S3XBKJcxdeagFJK+aUjfQIZwB6v5zmubT4ZY9KMMY8BE4wxt7R0nLX2CWvtZGvtZID+UYeuCVQ1RFNaUEtJiR+fQimlwlhHQsD42GZbOthaW2itvcZaO8RVWzi0yEj6Rhy6T2BnrgNbU8vy5W06q1JKKZeOhEAOMMDreSawt2PFaSYqynfHcLP7CeSXRhNNLXv2oJRSqh06EgJfAsOMMYONMdHAJcCizimWi8NBuvPQzUH7i6NxUKchoJRS7dTWIaL/AlYCw40xOcaYK6219cB1wLvARuAla+2GTi1dVBQ9G/IoK2u2vVkI7CuMxkE9uXucnfr2SinV3bV1dNCsFrYvBhZ3aokAY8y5wLljkpJIc+azfr0sIhcX5zrAKwRqamB/sSsMdtcBMZ1dHKWU6raCctkIa+0b1tqro+Pjia8qoKGugRUrvA7wCoEtW6DGRgOwf48OE1VKqfYIyhBoFBWFsZbeEYV8/LHXdncIREaycSPUIiGQn1uLbXF8klJKqeaCOwRcbf4njco7OAQcDjCGb7+FOlcINNTUUVBw+IuplFKhKrhDwDUj+JQj8/jiC6/7CrhDANi4EZLSJQR0mKhSSrVPcIeA60J/VFYedXV4+gWahUDvDPldQ0AppdonKEPAGHOuMeaJMtcssZHp+URGwkcfuQ5whUBDA2zeDH0GeGoCOTkBKrRSSoWgoAwB9+igpJ49ISKCmNI8rhyxnPMfPQPKyxtDYMcOqKmBflkSAvFRWhNQSqn2aOtS0oGTng5r1vCnHU+QUJlH5cq1xLlCYONGOSRjsIRARm+dNayUUu0RlDWBJnr3hsWLiaspAmDrh7saawLffiuHZA6LBSAr/YCGgFJKtUNohABQ+48FAOz9zBMCa9dC//6QMDwTgBHxezQElFKqHYI/BH7zG1iwgNifzKY4Kp2qTRIC1uHgvffg1FOBgQMBGBK1i9xccOoSQkop1SbB3ydwxhmNv1amZxH3/S7qqx1UV0dRWAjnngv06AG9e5PZsIu6OsjLg759A1dkpZQKFUFZE3APES0tLW2y3TE0i4Hsoji/jqIKB1FR8IMfuHZmZdGrcheANgkppVQbBWUIuIeIJicnN9meOj6LLHZRml9HQYmDadOg8ZCsLBKLNQRaVFUFV1wBezv3vj9KqdAWlCHQEsfQLOKowrl3H6VVDs7xvp19VhYx+3cDVkPAl2++gWefhSVLAl0SpVQQCakQICtLHuq3UcfBIWCqqxkYk8fu3YEpXlCrqJDH4uLAlkMpFVRCMgRiqCUm3sHQoQfvO2HgLjZ07v3Nuofycnn0CoGFC2HAAM/K3Eqp8BOSIQDQy7VoXPN9UzN3sWYNnXpfgSVL4NVXO+98AeEjBL7+GnJyID8/MEVSSgVeaIVAaiokJAAw/EjfITAuZRf5+XTqQnL33w+/+13nnS8g3M1BRUWNm9z3XigsDEB5lFJBIbRCwJjGi31kTLMQSEmBpCSyI2WE0Jo1nfe2JSXd4ELpoybgDgG9EY9S4SsoQ6CleQKAp0nI4fC5L71yFxERsHq1bGpogKefhupq/8vjDoGQnonsIwTczUAhH3BKKb8FZQi0NE8AgEGD5NFXCAwaRFTOLkaN8tQEXn0V5s6FN9/0vzwz9/6VO523UVLi/zkCzsfoIG0OUkoFZQi06hA1AXbtYuJET03ghRfkcedO/97OWph+4N/8iJdC+2KpzUFKKR+6XwiUljJlRAnffw8bNsDixbLL37kDVVXQ3+bSk6LQvlg26xiur/fkQUiHm1KqQ4J/AbnmDhUCwJR+u4AUfvtbGQOfkOB/CJQUW/qzl0gaKMi3gPHvRIHmrglUV0N1NcXlsY3DaEM63JRSHdL9agLA8B67MAYWLYIRI+CEE/wPgfIdBcRQSxQNlOWW+1noIFDuVfbi4iYXfq0JKBW+Qi8E+vSRaa7uDmJvrhCIy9vF8GFODE4uvVQ2+xsC1Vs9Ew4O5ITwkgvu5iBoEgIOh4aAUuEs9EIgIgJ27IBrrjl4X+/eEBsLN93Ehi0O1nMks2dZBg6UC92BA+1/u9oduY2/V+8tauXIIFdeDj17yu9Fnv6NoUO1OUipcBZ6IQAQGel7uzEyvXfOHMpOPJdRbCS7YUtjC5I/q4s693hCoHZ/CNcEyssb78BGcXHjHIHhw7UmoFQ4C80QaM3118Njj5Hy2H3yfNmyxmufP01CEfs8IeAs8K8mUFwMp5wC27f79fLOUVHRJATc3/6HD5fJcPX1ASuZUiqAgjIEWp0x3FbDh0NaWodDwJGXS517EJWfyzCvWgUffQRLl/r18o5zOn2GQHw8ZGbKpqIQbulSSvkvKEOg1RnDbWUMHH88LFtG//7SleBPCPQoymWLOQKAyFL/rpTuZqjvv/fr5R1XWSmPXlf8ggLo1QvS02WTNgkpFZ6CMgQ6zdSpsGULUUV5ZGT4FwIJJTnsij6CuohoHBX+1QTc77t/v+/9VVWdu/T1QdzDQ5OT5cdVE0hPl8oSaOewUuGqe4fA8cfL4/LlDBzoXwgkH8ilKC6D6h6pxFYV+7WInPt9fdUEigotV6X8h7df6sI5CO4QSEiQEUI+QuCw1QS+/baLE08p1R7dOwQmTpQho65+gXaHQGUlCXUllCVmUJvQk1SK/FpErrUQ2P/+Op6v/RH1z77Q/hO3lXuOQGKi3JPBKwQOa3PQ9u0wejS8/fZheDOlVFt07xCIiYGjj24MgT172rkcdK6MDKpMycCZlEoqxX41m7TWJ2CXLgPA7NrV/hM3U1QEs2b5uKC7awLuEHD1CRz25qC9e+UxoMOklFLeuncIgDQJrVlDdp8D1NZCXl47XusKger0TGxqKj0pavc3Zmtbrwn0+GoFADH7/ZzS7GXlCst3C1fz4YfNdng3B6Wm4iwqprxcQiAuTipLh6Um4B7t1VLniFLqsAuPEKivZ+yBlUA7m4RcIVDfJ4OI9J5+1QQKCmTNtvR0GWFaU9N0f89NywFILu14CER+sZLVTKbi/ZVNd7iagy7/eSLFpie2SDq409NlEFVa2mGqCWgIKBV0un8InHgixMUxbO3LQPtCwO5xrRuUkUF071S/lpN2v9/RR8tjk5pIbi7JxbtoIII+dXsaR3L6zdWkVLNha9PtrprAklWJfJOTiikpBiy9esnutDStCSgVrrp/CMTHw3nnkbrkZaKoa1cI1O3MpZQk4vskENO/J8mUUZTXvqm17v4Adwg0aRJaKd/Yl3AKmeSwc1tDu87dnM2TtSDqd+U23eEKgQoS+CYnlYi6WuKobOwUTk/XEFAqXHX/EACYNYuIokLO6/FBu0KgfncuuWSQkgLRvVMBOJBb0q63dr/fUUfJY5MQWL6cmohY3jDnE0UDe1fva9e5mzNFUk2JzstpOgrT1RzkDgGAVIoZsHs5/PjHpPd0anOQUmEqPELgjDMgJYW5cf/i/ffl5vNtYXNyySGTlBQwabICZ/W+phPGliyBv/2t5XPs3i0dr0ceKc+bhMCKFWyIPxrn4CEAFK31Y4U7L44SqQn0rs91d2eI8nLqHD1oIIpC6wmBfv95GJ5/nqGxOYe/JqBzBZQKCkEZAp2ydpC3mBiYOZMfHHiN7d9W8eyzLR+6caOsUl1fD1Hfe2oCpMrFs25/06UjHnoIfvUr6fz1Zfduuf1Bnz7yvPFLcGUlrFnD5xHHETdC1vSp2tyxzuHYMgmBDHLZvNlrR0UF1VEJxMdDVayEWW/yiPn4XQCy2U5RUTuHz/rD/d+zqqrp/Q2UUgETlCHQKWsHNTdrFo7qCm4Y+ha33SbXIV8efRQefxzWLyvBUbjPEwKutfidRU1rAps2QW2tLBLny549sm5bTIycorEmsGoV1NezpGYqEVkDALC72hAC1sI33/jcFVfpCYFNm7x2lJdTGZlI374wZLKE2UUxb2LKygAYUL8DpxO/JsK1i3eot2usrlKqqwRlCHSJk06Cvn2ZF/83cnMtDz/s+7B33oHhbGLQJceAMbzHD6QS4KoJeK8kWlPjmfe0bJnv8+3e7Vm8s08frxBwdwpXH0vygCQqopJxfN+G5qBFi2DsWJpe5UVitYRAX75ny0avDuzycipIpFcvGH+yKwTq/yW3FYuMpF+VfIgubxIqLZUxqaD9AkoFifAJgchIuOMOUtZ+wp/GP8cf/yg3KPO2dSskb13F5xxDZFkxr/xsCUs5sUlzUJTXSqLbtnn6F3yFQG0t7NvnCYG+fb1CYO1a6jOzKCKN3r2hLHkASaW7D91UvmGDPG7ceNCu1Lp86iKiicRJ3jqvzoeKCsqcCfTqBVPOks/Rq2E/TJsGAwaQVioh0OWdw6WlnntEawgoFRTCJwQArr4apkzhF7tuJJ0CZs6E6lLP7K1337E8xA1UR8ZzxahVfJt2AiALb7pDILrSs4icu919/HhYseLgNvXcXGm9GSCtPU1D4JtvqBg8FpAlnWv7DKR/w57GO361aNs2eWyWYLa+gVRbSF4v6YE+8J1Xz3B5OcUNUhMYcXQSDe7/7NOnQ3Y2SQU+agIrV8Ixx3Ru231pKRwhy3JrCCgVHMIrBCIi4IkniCwv5YvMC/j7V8cQndID/vxnAPKffZvjWc4nJ97Oe5sGUlgoKy1ERQHR0dRGx5NqPYvIuVtk5s6VVqLmX87dcwS8awL79yPtSJs2UdB/DCC3RjZZAxnI7kMvq+MOgWYHVuYUEYGlcMB4ACL35zbeU9mWl1NUKyFgIiOwySmy45xzIDub2H0+agKLFsEXX8DXXx+iQO1QWio3NQYNAaWCRHiFAMCYMTBvHmkbl9O/v2E5U7G//jV1r73JjFW3UpA0mOrZczlwQPpu3V0BAHWJsnTEPtdw/k2boH9/OOssed68Scg9R8A7BCoqoHLNJqivJ7enpybQ44gB9KKAXRsPMW3YffFvVhMo2yZViIqh4wHIJIfvvpN9tqyCMpvQOEM4Kj1VvpEPHQrZ2UQV5hFPRdNhpWvXyqO7+amjnE4oK5PpyWlpGgJKBYnwCwGAe+6BsjL67/6Mh896lzV2AhEXXcB4+xV7rryLcUdFA/Dll0h/gEtUuiwd4b5N5ObNMGIEDBkinb7Llzd9G3cIuJuD3MNEy5evA2BbvIRA796QPEaSomhdTsvlrqmBHNf+5jWBXRICDUeMxOmIbjJM1JaXU+7qGAZkTOvvfy+/Z2cDMKnnTposZOoOgfXrWy5Pe1RUSNtYcrL8ITQElAoK4RkCxkBCApGRsOA/cdwx7r/kOdNZz5EMu2M2o0ZBdLTMFfAOgeh+PekbU8y778r1bNMmCQFj5CZmzWsC69bUM7PXp8TNvw/mzCHLIUsp169ZBzExfGeHERMjTU4xQyUpDmxsZZjozp3yxn36yO9enRDVeyQEYgb0hv79PcNErSWisqJpCFx7Lfzwh/L74MEATO653TObuqDAs+xzZ9UE3MNDNQSUCirhGQJe4uPhqfcyOT97Pfef/QkJyZE4HJ4Zvt4hYFJTyYwrYskS+UJeWir3swdZrHTHjsaFR2logMlv3cnL+dPglltgwQJGfPIYAJEbv4HRo9lfGCX9AYbGNqNWJ4y5+wNOO01mp3lNP67dJw36cVm9iMjMYEisqyZQXY1paKACT3NQE66awJFx2z01gXVSU6nPzOq8moCGgFJBKexDAKQ5ZsXmNP7xSs/GbRMmyKN3nwCpqfQ0xZSV0TjreMQIeTz5ZHl86y15XLMGplZ/SOHgSZCfD6eeSvp7LwKWxB3rYMwY8vLwXJgzMrDGUL9zT2OH8kG8QwCa9As4v5eaQFJ2OmRmMjAiR2oCrsXjykmkd28f50xLg8REhkZITcBaGkPgT3tnS9k7Y2KXdwj07q0hoFSQ0BBwiYqSNX7c3CHgXROgZ09iK4uIiIC//102uUNg3DipFbz4ojz/aHEVk1hN7PRTZZnO2bOJ2rmNc8xi4kv3wdix5OfjuTBHR9PQqy8D2c3LL7dQyO3bpepyzDHy3CsETEE+xaSQ2tsBGRn0qs3lu80WZ5kM8WzSHOTNGMjOJrN2O5WVrmGia9dSFteH952nyDGd0STUvCZQXt7ytG2l1GGjIdACnyGQmoqpruaEyVXk5kKPHpC59WN47jmMgdmz4dNPZWjontdWEU0d8adPlddeeCFER/PHyN/J87Fjm9YEgKghgzgh9kteeamFFe62bZPmG1c7vnfncGRRPvn0IiEBuf9BfRXRVSXs3yo1gfqYBHr0aOHDZmeTVibn2r0bWLeOLXHj2MBo2d8ZTULNQwACs3REYaGMjPrqq8P/3koFIQ2BFowbJ9/SR43y2uhaP+jc42XpiOt7LyTijNPh8sthxQpmz5bmlCefhKRvXEOFjjtOHlNSYPp0jqx3jbppXhMAuO46jqj+himf/cV3k9D27TIUKTZWxqZ61QRiyvIpjUqX/oWMDEDWENrzrYRAZEpiyx928GAS8ncAlt3b62HDBlbXjeV7+lIR07NragIQmCah9ethy5aDh3IpFaY0BFoQHy9LPlx8sdfGfv0AuO6FKTzNHP6wa7Zc5AcMgGuuYWhWHUcfDfffD1OcyzkwYDiNd24BqSoA39OHldt6U1nZtCbArFlUnHo+v+dWljy6icJCuPtu1xd+a+UXV0cu2dlNagI9KvIpi3WdLDNTHshh3xZpDnL0bCUEsrOJqKmmL99Tvvo7qKlhaek4wLA1enTX1QQCEQLuIbY5rQzFVSqMBGUIdPpS0n6KaP7XOfdc+Pe/cUw4kh/zHDlHngVvvw3z58vKnvPnM3s21FQ7OY4VxJwytenrp0/HmZDIejOGP/1JNjWpCRhDwnN/pyYyjjF/+Qkjhzu54w7XhOZ9+6QNfYjce4DBg5vUBBKq8jnQwxUCrprAsB655G+XmkBsekLLH9QVLKNitmPWSU3la8bRqxesqjkSu2FDx9f/Ly2VjpcePTQElAoiQRkCXbKUdGeIiIAf/YiItxcTWVzIwLVvQFwcnH++BMQddzD7mG2MNJtJo4ioE5uFQI8eRLz8HxafeD+vviqbDuqs7dePpRf8hYk1n3FVz1eYPBnefx/Pt353CGRny4WspgasJamugOpE18n69wdgTM9cSvZICPTo3UpNwHXO30fezrCvX6Ih0sFmhjNjBqypPRJTUuKZN+Cv0lKpBRjjST4NAaUCLihDICSkpHiqCsbI7cViYuh15Xk8edHbsn3q1INfd8YZHH/9xMYv1r6GbZ6+4DIODBzB72PvYfYlTr77Dgq/cA0PdTcHDR4s385374bSUhy2jroUVwhER0NGBmcdeBlHjoRHfN9WQmDYMPjDHxhVt5Zj9r5Ofq9R1JtoZszA0znc0X4BdwiA9GkkJ2sIKBUENAQ6y8CB8PLL8N13THn1N9IX4F4xs5np0z3zD3yFQExcJPF/uBXzzTdcGPE6AHs+3iah416K2R0GO3bgXnrUmeZVrXjiCfoe2Movyu8FIKl/K81BxsAtt/Dby/bws6TneOzIR8jKgsmTYT2uWXMt3MimzbxDAJrdXOEw8g4BvcWlUhoCnerkk6VG0NAgHcbuG6g0ExMDl1wi13SfY/dBeqSHDWPggruZnrqCnktflw7oaFnXqCBRhokuXbCdBtdEsSYnO/tslt/5AUWkUkUs6f2jD1n8/kN68Peyy/h3zlRGjJDTmbQ0iuIzZfZbRzQPgWHD4NtvfR/73HOwYEHH3q8le/bIH76m5jDcRUep4Kch0Nl++lP45z/h9ttbPezee6VPOT6+hQOiouDWWzFr1/Jm8VQSS3Nw3nk3AHV18MNf9KeCeOy771G5W5aMiOrXNFF6zziOo/mCH/FSy2Hjxb3a6aZNMHKkZNioUbA+ZlLnh8DEibL2dqWPVVPvvFOGWHW22lppgnKvCaJNQkppCHSJK6+ESZNaPSQ1FX7wg0OcZ/ZsuPZavpj1FwbY3ayfeDkAN90EH38awYK+N3Ni4Ws0PPUMANEZTa/0Q4bAzsihvMm5bQoBd0sTeGZCjxoFyysnYjdvblyCwi++QsDpPLiZKS9POsG3bpUV/DqTu3N7yhR5bHF9DqXCh4ZAMIuKgkcfpf8DN3CABJ5+Gn78Y3j4Ybj+ehj59G/4lpGkLHkNgB4Dm17pY2I8k4vbUxMATwiMHg3LqidhrO3YDWaah4B7SnbzGsZnn8ljXR1N17buBO5v/u4Q0JqAUhoCoSAzUy7KDz0kfc/z5sGDD8JxJ0VzffTjAFQQT3Lfg9eFcF/M2xICGRmeAU/eNYHVuGo1q1f79wGslRvKeIfAwIEyA7t5CKxc6fndfUOEzuK+6E+aJAGrIaCUhkCouPNO+OUvpZXkvvvA4ZCRllEnn8BfuIEvONq9qkUTkybJCKQW+x68OBwyxSA11RMao0bBfvpSkdTP/36Bigpp+vEOAWOkNtB8DZ/PPvOMfHLfGq2zuC/6WVnyQTUElNIQCBUXXywzh12TgRudfjrcyJ85lQ+bLnvtcvPNcpOwFgYqHWTYMLkDp/v4vn0lELYmT/K/JuC9ZIS3iROlT6CuTp7X18t9jadPl1pCV9QEEhIgKUmqVxoCSmkIhLrTTwcwgPEZArGxciFvq6ef9twrATxf2D+rnSTDhtx3r2/u+edlyJMvPkJgyxYoGjRRRuy4h4quXy+jhY49VuZYdEUIZGbKh9IQUArQEAh5Y8bIvKu4OOkI7qisrKajhEC+sL9bMEmadHx1Dv/f/0mP9a23+r5wNwuBmho48US4+d/NOofd/QFTpkgIdEVzkGtxvcYQ0AljKsxpCIQ4Y+Cccw6+cHemiRPhi4aJ8sR9wa6vhy+/lGFKv/kNnHeedLY+8cTBJ2gWAv/6l0wWfm39MGxCguecn30miTZokNyhJzfXc4P6efM6PleheQhUVkJxccfOqVSIiwp0AVTHPfywXCu7yoQJsJf+VCb1IW7hQli2DN55R0b8gEyQe+QRmQb9zDPSLOR9mzavELBW+jaMgYKiCGomjyfW3Tm8cqU0BRnjWXJjyxa5WD/wgDQbvfGGfx+ivl5WYvUOAZBg8NWjrlSY0JpANxAf71mduStkZ0NSkmFL2hRYsQI+/hh+9CP5Sr93Lzz2GLUNkRy47KdQVMRB98f0CoH335e+4J//XDbt6TUBPv9cOi62bPGM4R8+XB43b4YXXpDfFy8+eDVTp1OaoubN84RSba28iXdTz/ffy7G+QkCpMKY1AXVIERFSG/ht+WO89a9bZGW5yMgmx/z0p/DJR6ewbehQzGOPSZPOG29AYqJn0ldyMn/+s1zv//AHePxxeLfXjxn2g21yw57sbLj6ajl26FCpEWzYAC+9JPdV/vxzWVPolls8b/zqq9IpDdKjffrp8Oab0szz1FPwk5/IPvfFXkNAqaastUH7M2nSJKuCww03WNujh7X19Qfv27HD2shIa8HaTVc+IL+AtQ6HtcY0/v7NOqcFa++9V143aZK1p57ayptmZVnbt6+8ftEia0880dqhQ611OmV/Q4O1Rx5p7YgR1q5cae2UKdYmJlp76aXWTphgbe/e1paWyrH/+Y+c5+uv5XltrbUREdbedlsn/YWUCg7AKtuO66w2B6k2mThRbmzma/DPn/4ktYWkJHiw/Kfw61/DwoVQUCBrAf373/Dvf/OXhww9ekitAaRCsWpVKwN0jjhCmnF69oQzzoC5c2W23NKlsv+112RY6W23STPSypXS9PT889JBnZ8P99wjx7pvyuOuATgcUiXRmoAKd+1JjMP9ozWB4LF+vXyRfu65ptvz8qSG8JOfWPvTn1obF2dtefnBr9+3z9roaGt/9jPPtn/8Q865ZUvTY7/6yto1a6y1110nB1xzjeyoqJBv+iedJDWDsWOtHT7cd/XEWmvnzpXayKxZ8q1/yBBPLcJaa087zdrsbKlRKNVNoDUB1RWGD5fbAzdf5eFvf5Mawk03Sf9sZSWNt8709sgjMjH4hhs82yZPlsdVqzzbnE644AK46CKwI0bKxksvlcf4eJkC/emnMiR13TqZm9Csf6LRvfdKoV99Vdbc+OyzplOn/+d/pIbwzjvt+lso1Z0YG4STZYwx5wLnDh069KotW7YEujjKZcoUaZ25+WZZYO7ZZ2XgzllnweuvS7PO0KHSv/v++57XVVbKenHHHy/HudXVSb/xddfJgnggr3Mvsf31sgrG5b0PM2Y0vXgfOCCT1vbtgwsv9Kx658vWrTKTznXf5SZqa6UDe/x4GXmkVDdgjFltrZ3c5he0p9pwuH+0OSi4vPiitYMHe/p94+OtvfZaaepxu/126QvesMGz7a675PhPPjn4nMccY+20aZ7nP/yhtSkp0tF8yy0HH//cc9b+/ved9pGsvfNOKXDzNimlQhTtbA4KypqA2+TJk+0q77YCFXDWyhJC69fLaMyUlKb79+yRTuSGBnjlFflm/8c/SvPOSy8dvJDdddfJqM89e6RmkJEhcwg2bJDbJ3/3nec19fVyh828PBl16u7j7ZB9+6Sacv310sMNUpDXXpMaxPTpbV99T6kgoDUBFXDbtlk7cqSnxnD11dbW1fk+dulS6budOFG++YN0Qj/+uPy+dq3n2Dff9Jzz7rs7scCXXCK92zNmWHvTTdYOHOh5o5kzrd261donnrD2jDNkf06O57UVFdYWFVmbn+/pYHY6rX3pJenAfumlpp3RLamqsvaLL6z96CNr339fngeThgap3vn6LFVV8vdZtMjazZvb9nlVl6GdNYGAX+hb+9EQCF0lJdZedpm1999/6GvCW29ZGxMj/zcee6xs279fBvTceqvnuJkzrU1Pl2vrwIEtDwpqt127pLDDhkkhjj/e2jfekMJHR3sCYfBgKZTDYe348dJu5d4HMi/hiitk1BFYm5Agj+efb+2nn1q7Z4+1GzdK+9ixx8r23/5WRi+5j3X/TJhg7fbtUj6n09pNm6x9/nlr582zdvHig/+oOTnWTp9u7c03W1td3XSf02nt8uVyDn/ddJOU66yzJBR97XP/zJhhbU2N/++lOkRDQIWkDz6wtlcva195xbPt5JNlBKjTKV+0HQ6ZtOae97V4cdNz1NTIkFVfnE7fQ1cP0vzitW6dVDs+/1xOsm2btddfL7WCa6+19r77rP3LX+Rn1ixrU1OtTU629m9/k4vxAw9YGxvb9CJpjHSGjBwpnR/p6dZedZV8+CVL5GKfkiLnmjPH2oyMpq8FmTj33//KTL2lS63t08eTpGPHWvvOO3Kup5+WwHK//sILrf3wQ7mQ791r7QsvWDt7trW/+IW1u3f7/pu8957nPRMT5X0ef1z2ffONtVFR1l5+uUzYu+MOz/vU1jb9D/Ddd9aWlXm2VVVZO3/+waHiVl6utQo/aAiokNX837u7Sejyy639wx9sY/NQTY186Z4xw3Nsebm1xx0n18iTTpI5CMXFsm/LFtkH1h5xhMxp+Oc/5Ut5p19j6usPbvvat8/at9+29rHHrH300abNSTU1vqs0W7fKlOqkJKkCPfGEBFJlpbWPPCIXfe9gGTpU2tHeeEP+ON77RoyQ1992mwSU9z6Q9HU45GfmTGvPPdfaE06w9sYbpWmqb19rR42y9sABa3NzrT3zTHndH/8otaa0NGsLCjxlf/hh2T9livzHu/RSazMzZVtGhrWffSZVxWnTZFtsrJwrN1cmiDz1lEwlN8baX/+6k/8DdX8aAqrbqK+XL5buL7/e/zvMmydfoufPl2//J54oz6+91tOqEx0trRdxcfLFet48ub6lpXmuf96T14JSSxPZDhyQJqbHH5cLaFGRZ19hodQEPvrI2tWrm56jpETa3xYskIv1ypWyf+dOmZQ3cKDUJI47TkIB5Ju/d+dMba30o7j/iE89dXD5HnnE2tGjrR00SC78M2da+9BD0qQWHS2h5XBIjenCCw8Opuxsa08/XX531zpa+vsUFUkNw7vm0R7dobaxebO1L79srW1/COjoIBX0liyRUUR33imLlwLs3y8jjpYt80wTeOEFWc3aWrkT5osvyjyxI4+Exx7zjCayVkYd/e538NZbcq6kpIB8tOBWXAyLFsnifu7JG24NDXIfif37ZcJIa3M1vBUVyeS/Zctk+Jj7vO+9J6vIuhcSHDdO3uO882TfXXfJKrEFBXLj69GjZfmQZ5/1LP0RGSl3K7rwQs/Nta2VoWbr18vPt9/KkLaTTpL9b7wBH30ksyFPPx2OOw5GjpT/WYqL5f2cTjl3ZaWMJquqgjPPlNdXVMhChevXy9C1wYNleNzw4bJv+XJ5PPtsubWpm7Xyedz3325okPtn7Nol54+L8/2TlyezK7dulffLyJC/46JFkJ4OOTmYmJh2jQ7SEFAhbcUKePRRuVa4A6KtVq6Uf/PPPANXXNElxVO+WAvV1TKb+1DKyuCEE2R2uMMBqalyIQQJnjPPhFNPlQtpXp6sILtxo+9zDRokAZKXJzcocjphyBBZl2rzZgmmmpq2fYaoKDj5ZPm2UVQk61sVFXn2x8dLaLivr/HxMty4pESCaN8+ufD7KynJs3R6z54yrvrnP4c+fdo9RFRDQIUt65rhPGSIfNlUQaquTi6aGRnyjTw/X755Dx/ueyb4li2wbZvUUpxOufCPGiXT091KS+U8Q4Z45oFUVckFeuNGeb+ePeXbdVSUTFKJjZWaSn29LIr46qtSI5k3T26GVF0ty5CsWiU/aWkSYFFR8NxzUu3s21dek5UlgZaYKJ8pIkLOnZUlgVFVJSHS/CcpSWo5fftCebnUHLKzpZbgoiGgVDvcdpvc2yA3V/5dKRXq2hsCuoCcCmuXXipfFhcuDHRJlAoMDQEV1kaMkH68p56SRUbz86UVYfNm2L27lXsdBImtW+HKKz3N5Eq1l95eUoW9n/9cLqTHHnvwvvh46TdISvLcy3nAAGmCLSyUBU2zsuT+N0ccIcfGxh6ectfXw2WXyV03IyPlPjpKtZeGgAp7c+fKyMJNm6Q/0eGQUYTl5dJPuH27jOQrKJD+yL17pQkpLk4GuBQWes5ljARFUpL8JCbK44ABMGaMjDwsL5fw6NtXQqNPHwmYqHb+a3zwQQmASZPgySdlGO3YsZ36p1FhQENAKeRiPHRo246tr5cBK+4RjmVlMiBlyxaZf7Bnj1zoy8rkcft2+PBDCZLWOBwSBvHxMuBj9Gh5TEuT99q3TzqwU1Jk2x13wMyZUgMYOhR+9SsZ5aSLnqr20NFBSh0GTqf0MezbB8nJUovIzZU2/YKCpqMA3aGyYYMMK/fWo4eMHgSZq7R+PfTqBfPnwy9+AT/5iTyvqZEwys+XkZXDhsmIR2MgJkZGPqalyfOGBgmeXr3knAkJbQsSa6X/ZOdOGSI/bZqcRwWWDhFVqpuwVmoPRUUSDv36SYBUV8vw8ORk2QZSMznnHOncrqmRWkVmplzYc3LkeKezbe8bGyuv69FDmqgcDvlx/x4ZKXeY27lTyuKWmAizZsFpp8m8rIQETxBNmCATcSsr4e23JRBnzJCajupcGgJKqYPU1HgmsFZXSz9GYaE8j4yUPoq8PLlgux+rqz1NX3V1nt/r66UmMXiw58fhkGU6XnrJU1Nprm9fqdl4B8cxx0iYlZVJ6AwcKOcuKZEy/exnMt9KtZ2GgFIqYA4ckM71HTvk9wEDpA/j88/h449lkuzMmVJTWLgQ/vtfeV1iohy/a5c0j6WmSuCUl8PTT8Ps2QH8UCFGQ0Ap1S0UF8MFF8Ann8i6UCkpnmapqCiIjpbn0dEH/7Rlu9MpNZ7iYhlh1daBAcGuvSGgo4OUUkEpNRXefRf+93+lH8G7OcrdRFVX13nvN2KELF7q3f/hz2NkpO+fqChp8oqPlxCKiJAO+IgI+YmLk36U2NimrzvUAq3WSu2poED6XdpLQ0ApFbRiYlqfBGetJwxqaw/+8bXdvQ1kNFR8vKxK/eab8PXXB/d/NA+etnawd6bmgeJweOap7N8vzWYDB0pzWntpCCilQpYxnuadjgxPHT9eahxt4XRKKLQUFA0Nvn/q66Vz/sABOc5aOZe1sr+yUi7mNTUtn8P9U1fnGVKcni5NWcOG+ffZNQSUUqodIiI8wdMd6AJySikVxjQElFIqjGkIKKVUGNMQUEqpMKYhoJRSYUxDQCmlwpiGgFJKhTENAaWUCmNBvYCcMaYc2BzocvgpHSgIdCH8pGUPDC174IRy+ZuXPcta26utLw72GcOb27MaXjAxxqzSsh9+WvbACOWyQ2iXv6Nl1+YgpZQKYxoCSikVxoI9BFpZRDboadkDQ8seGKFcdgjt8neo7EHdMayUUqprBXtNQCmlVBfSEFBKqTAWlCFgjDnTGLPZGLPVGHNzoMvTGmPMAGPMR8aYjcaYDcaYX7i29zTGvG+M2eJ6TA10WVtijIk0xnxljHnT9TyUyp5ijHnZGLPJ9d/g2FApvzHml67/Z9YbY/5ljIkN1rIbY54yxuQZY9Z7bWuxrMaYW1z/fjcbY84ITKkby+Kr7P/n+n9mnTHmNWNMite+oC67175fG2OsMSbda1u7yx50IWCMiQQeAc4CRgGzjDGjAluqVtUDv7LWjgSmAD93lfdm4ENr7TDgQ9fzYPULYKPX81Aq+8PAO9baEcA45HMEffmNMRnA9cBka+2RQCRwCcFb9meAM5tt81lW1///lwCjXa951PXvOlCe4eCyvw8caa0dC3wH3AIhU3aMMQOA04HdXtv8KnvQhQBwNLDVWrvdWlsLLATOD3CZWmSt3WetXeP6vRy5CGUgZV7gOmwBMCMgBTwEY0wmMB34p9fmUCl7EnAi8CSAtbbWWltCiJQfmazZwxgTBcQBewnSsltrPwWKmm1uqaznAwuttTXW2h3AVuTfdUD4Kru19j1rbb3r6WdApuv3oC+7y1+A3wDeI3v8KnswhkAGsMfreY5rW9AzxgwCJgCfA32stftAggLoHcCiteYh5H8mp9e2UCl7NpAPPO1qzvqnMSaeECi/tTYXeBD5JrcPKLXWvkcIlN1LS2UNtX/Dc4G3Xb8HfdmNMecBudbatc12+VX2YAwB42Nb0I9jNcYkAK8AN1hrywJdnrYwxpwD5FlrVwe6LH6KAiYCf7fWTgAOEDzNJ61ytZ+fDwwG+gPxxpjLAluqThMy/4aNMb9DmnRfcG/ycVjQlN0YEwf8Drjd124f2w5Z9mAMgRxggNfzTKSaHLSMMQ4kAF6w1r7q2rzfGNPPtb8fkBeo8rViKnCeMWYn0ux2ijHmeUKj7CD/r+RYaz93PX8ZCYVQKP9pwA5rbb61tg54FTiO0Ci7W0tlDYl/w8aYK4BzgEutZ8JUsJd9CPLFYa3r320msMYY0xc/yx6MIfAlMMwYM9gYE410dCwKcJlaZIwxSJv0Rmvtn712LQKucP1+BfDfw122Q7HW3mKtzbTWDkL+zkustZcRAmUHsNZ+D+wxxgx3bToV+JbQKP9uYIoxJs71/9CpSH9SKJTdraWyLgIuMcbEGGMGA8OALwJQvhYZY84E5gHnWWsrvXYFddmttd9Ya3tbawe5/t3mABNd/xb8K7u1Nuh+gLORHvttwO8CXZ5DlPV4pMq1Dvja9XM2kIaMmNjieuwZ6LIe4nOcBLzp+j1kyg6MB1a5/v6vA6mhUn7gLmATsB54DogJ1rID/0L6LupcF54rWysr0mSxDVkK/qwgLPtWpP3c/W/2sVApe7P9O4H0jpRdl41QSqkwFozNQUoppQ4TDQGllApjGgJKKRXGNASUUiqMaQgopVQY0xBQSqkwpiGglFJh7P8BVLJ5ylfzkEYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model.history.history['loss'], label=\"Loss\", color='blue')\n",
    "plt.plot(model.history.history['val_loss'], label=\"Validation\", color='red')\n",
    "plt.legend()\n",
    "plt.xlim(0,142)\n",
    "#plt.ylim(0,10)\n",
    "plt.yscale(\"log\")\n",
    "plt.title('Loss - 1D CNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334/334 [==============================] - 21s 63ms/step - loss: 0.0173\n"
     ]
    }
   ],
   "source": [
    "testGen = DataGenerator(testing, batch_size=1)\n",
    "errors = model.evaluate(testGen, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write test file names to an output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outFile = '/efs/tnarock/regression_noise_y0_revised.txt'\n",
    "\n",
    "f = open(outFile, \"w\")\n",
    "for t in testing:\n",
    "    line = t.strip() + '\\n'\n",
    "    f.write(line)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml-env]",
   "language": "python",
   "name": "conda-env-ml-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
